{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granite Dells PBR segmentation validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_pc_path = '../../data/granite_dells/associations/semantics.las'  # from ground truth 2D segmentation\n",
    "#gt_pc_path = '../../data/granite_dells/updated_point_cloud.las'  # from sam2 2D segmentation\n",
    "assert os.path.exists(gt_pc_path)\n",
    "gt_pc = laspy.read(gt_pc_path)\n",
    "\n",
    "# read intensity values of prediction point cloud\n",
    "prediction_semantics = gt_pc.intensity\n",
    "\n",
    "# read intensity values of ground truth point cloud\n",
    "gt_semantics = gt_pc.user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique semantics in prediction point cloud: 38\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37], dtype=uint16), array([     688,     1781,     2065,     2257,     2393,     2425,\n",
      "           2459,     2527,     2583,     2664,     2799,     2804,\n",
      "           2861,     3221,     3231,     3277,     3568,     3890,\n",
      "           4964,     5005,     5259,     5524,     5994,     6115,\n",
      "           6610,     9126,     9152,    10490,    11267,    12020,\n",
      "          12050,    14187,    14592,    15651,    17195,    24906,\n",
      "          26069, 12195635]))\n",
      "Number of unique semantics in ground truth point cloud: 37\n",
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36], dtype=uint8), array([12178955,     8469,    12292,     1987,     3558,     7509,\n",
      "           9514,    16066,    12699,     5307,    11294,     2725,\n",
      "          10568,     5719,     3400,     2903,    25429,    11695,\n",
      "           3770,    26047,    13960,    17963,     2373,     3283,\n",
      "           3389,     2335,     2357,     3258,     3027,     3802,\n",
      "           7638,     2947,    14372,     3148,     5793,     2651,\n",
      "           7102]))\n"
     ]
    }
   ],
   "source": [
    "# print the number of unique semantics in the prediction point cloud\n",
    "print('Number of unique semantics in prediction point cloud:', len(np.unique(prediction_semantics)))\n",
    "# print unique numbers and their counts\n",
    "print(np.unique(prediction_semantics, return_counts=True))\n",
    "\n",
    "# print the number of unique semantics in the ground truth point cloud\n",
    "print('Number of unique semantics in ground truth point cloud:', len(np.unique(gt_semantics)))\n",
    "# print unique numbers and their counts\n",
    "print(np.unique(gt_semantics, return_counts=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantics mapping: {(1, 3): 0.5878634639696586, (2, 26): 0.8004885993485342, (3, 28): 0.735873850197109, (4, 31): 0.7794068643785405, (5, 25): 0.8903891977760127, (6, 22): 0.9113924050632911, (7, 27): 0.650028522532801, (8, 4): 0.6088551218234215, (9, 35): 0.8474104970455335, (10, 15): 0.8500973393900065, (11, 11): 0.8338308457711443, (12, 29): 0.7501970055161544, (13, 14): 0.8927958833619211, (14, 33): 0.8351553509781358, (15, 23): 0.8904899135446686, (16, 24): 0.8192991631799164, (17, 18): 0.9054726368159204, (18, 9): 0.8967682363804248, (19, 13): 0.8290977315367559, (20, 5): 0.6493993024157085, (21, 30): 0.7162602686139001, (22, 34): 0.895016077170418, (23, 36): 0.8544969833029326, (24, 6): 0.6512032770097286, (25, 1): 0.8027663934426229, (26, 8): 0.7084440969507427, (27, 10): 0.8344421052631579, (28, 12): 0.8290333389177417, (29, 17): 0.8466749727456783, (30, 2): 0.8680070600874837, (31, 20): 0.9600974930362117, (32, 32): 0.8693687879179037, (33, 7): 0.8705472988912479, (34, 21): 0.92551618379977, (35, 16): 0.9237531052933308, (36, 19): 0.8964375386630763}\n"
     ]
    }
   ],
   "source": [
    "# associate the semantics of the prediction point cloud to the ground truth point cloud based on IoU\n",
    "# create a dictionary to store the mapping\n",
    "semantics_mapping = {}\n",
    "\n",
    "\n",
    "N = len(np.unique(prediction_semantics))\n",
    "# iterate over the unique semantics in the prediction point cloud\n",
    "for pred_sem in range(0, N-1):\n",
    "    # get the indices of points in the prediction point cloud with the current semantics\n",
    "    pred_indices = np.where(prediction_semantics == pred_sem)[0]\n",
    "    # iterate over the unique semantics in the ground truth point cloud\n",
    "    for gt_sem in range(1, 37):\n",
    "        # get the indices of points in the ground truth point cloud with the current semantics\n",
    "        gt_indices = np.where(gt_semantics == gt_sem)[0]\n",
    "        # calculate the intersection of the two sets of indices\n",
    "        intersection = len(np.intersect1d(pred_indices, gt_indices))\n",
    "        # calculate the union of the two sets of indices\n",
    "        union = len(np.union1d(pred_indices, gt_indices))\n",
    "        # calculate the IoU\n",
    "        iou = intersection / union\n",
    "        # if the IoU is greater than 0.5, associate the semantics of the prediction point cloud to the ground truth point cloud\n",
    "        if iou >= 0.5:\n",
    "            semantics_mapping[(pred_sem, gt_sem)] = iou \n",
    "            \n",
    "\n",
    "\n",
    "print('Semantics mapping:', semantics_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9459459459459459\n",
      "Recall: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "def validation(iou_threshold):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "\n",
    "    for pred_sem in range(0, N-1):\n",
    "        detection = False\n",
    "        for gt_sem in range(1, 37):\n",
    "            if (pred_sem, gt_sem) in semantics_mapping:\n",
    "                detection = True\n",
    "                if semantics_mapping[(pred_sem, gt_sem)] >= iou_threshold:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    FP += 1\n",
    "\n",
    "        if not detection:\n",
    "                FP += 1\n",
    "\n",
    "    # count false negatives\n",
    "    for gt_sem in range(1, 37):\n",
    "        missing = True\n",
    "        for pred_sem in range(0, N-1):\n",
    "            if (pred_sem, gt_sem) in semantics_mapping:\n",
    "                if semantics_mapping[(pred_sem, gt_sem)] >= iou_threshold:\n",
    "                    missing = False\n",
    "                    break\n",
    "        if missing:\n",
    "            FN += 1\n",
    "\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "precision, recall = validation(0.6)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU list: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "Precisions: [0.972972972972973, 0.972972972972973, 0.9459459459459459, 0.8918918918918919, 0.8378378378378378, 0.7567567567567568, 0.7027027027027027, 0.43243243243243246, 0.13513513513513514, 0.02702702702702703]\n",
      "Recalls: [1.0, 1.0, 0.9722222222222222, 0.9166666666666666, 0.8611111111111112, 0.7777777777777778, 0.7222222222222222, 0.4444444444444444, 0.1388888888888889, 0.027777777777777776]\n",
      "Mean precisions: 0.6675675675675676\n",
      "Mean recalls: 0.6861111111111111\n"
     ]
    }
   ],
   "source": [
    "def average_validation(iou_list):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for iou in iou_list:\n",
    "        precision, recall = validation(iou)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "iou_list = np.arange(0.5, 1.0, 0.05)\n",
    "print('IoU list:', iou_list)\n",
    "precisions, recalls = average_validation(iou_list)\n",
    "print('Precisions:', precisions)\n",
    "print('Recalls:', recalls)\n",
    "mean_precisions = np.mean(precisions)\n",
    "print('Mean precisions:', mean_precisions)\n",
    "mean_recalls = np.mean(recalls)\n",
    "print('Mean recalls:', mean_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine filtered point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in the point cloud: 12459304\n"
     ]
    }
   ],
   "source": [
    "pc_path = '../../data/granite_dells/associations/semantics.las'\n",
    "assert os.path.exists(pc_path)\n",
    "pc = laspy.read(pc_path)\n",
    "# print the number of points in the point cloud\n",
    "print('Number of points in the point cloud:', len(pc.points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in the segmented point cloud: 336898\n",
      "Unique semantics in the segmented point cloud: [155 222 230 232 240 281 284 311 318 323 336 361 364 369 378 418 422 444\n",
      " 445 450 451 455 458 460 471 480 512 522 536 548 554 560 565 593 618 620\n",
      " 630 644 650 663 673]\n",
      "Number of unique semantics in the segmented point cloud: 41\n"
     ]
    }
   ],
   "source": [
    "segmented_pc_path = '../../data/granite_dells/semantics_gd.las'\n",
    "assert os.path.exists(segmented_pc_path)\n",
    "segmented_pc = laspy.read(segmented_pc_path)\n",
    "\n",
    "# print the number of points in the segmented point cloud\n",
    "print('Number of points in the segmented point cloud:', len(segmented_pc.points))\n",
    "# print unique semantics in the segmented point cloud\n",
    "print('Unique semantics in the segmented point cloud:', np.unique(segmented_pc.intensity))\n",
    "# print the number of unique semantics in the segmented point cloud\n",
    "print('Number of unique semantics in the segmented point cloud:', len(np.unique(segmented_pc.intensity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique semantics in the point cloud: [0]\n"
     ]
    }
   ],
   "source": [
    "pc.intensity = pc.intensity * 0\n",
    "print('Unique semantics in the point cloud:', np.unique(pc.intensity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Combine coordinates\n",
    "pc_points = np.column_stack((pc.x, pc.y, pc.z))\n",
    "segmented_points = np.column_stack((segmented_pc.x, segmented_pc.y, segmented_pc.z))\n",
    "\n",
    "# Build nearest neighbor index\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='kd_tree')\n",
    "nn.fit(pc_points)\n",
    "\n",
    "# Find nearest neighbors with distance threshold\n",
    "distances, indices = nn.kneighbors(segmented_points)\n",
    "epsilon = 1e-3  # Adjust based on your data precision\n",
    "\n",
    "# Create masks and flatten arrays\n",
    "valid_matches = distances.ravel() < epsilon\n",
    "valid_indices = indices.ravel()[valid_matches]\n",
    "valid_intensity = segmented_pc.intensity[valid_matches]\n",
    "\n",
    "# Safe assignment\n",
    "if len(valid_indices) > 0:\n",
    "    pc.intensity[valid_indices] = valid_intensity\n",
    "else:\n",
    "    print(f\"No matches found within {epsilon} distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-arrange pc.intensity to start from 0, 1, 2, 3, ...\n",
    "\n",
    "# get unique semantics in the point cloud\n",
    "unique_semantics, counts = np.unique(pc.intensity, return_counts=True)\n",
    "# create a dictionary to store the mapping\n",
    "mapping = {}\n",
    "# iterate over the unique semantics in the point cloud\n",
    "for i, s in enumerate(unique_semantics):\n",
    "    # exclude the semantics 0\n",
    "    if s != 0:\n",
    "        # map the semantics to the new values\n",
    "        mapping[s] = i-1\n",
    "\n",
    "mapping[0] = len(unique_semantics) - 1\n",
    "\n",
    "# map the semantics to the point cloud\n",
    "pc.intensity = np.vectorize(mapping.get)(pc.intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new LAS file\n",
    "header = laspy.LasHeader(version=\"1.4\", point_format=3)  # Use version 1.4 and format 3 for intensity support\n",
    "\n",
    "# Create a new LAS object using your existing data\n",
    "las = laspy.LasData(header)\n",
    "\n",
    "# Populate coordinates\n",
    "las.x = pc.x\n",
    "las.y = pc.y\n",
    "las.z = pc.z\n",
    "\n",
    "# Add intensity (if not already present)\n",
    "if hasattr(pc, \"intensity\"):\n",
    "    las.intensity = pc.intensity\n",
    "    las.user_data = pc.user_data\n",
    "else:\n",
    "    # Create new intensity field (scaled to 0-65535)\n",
    "    las.add_extra_dim(laspy.ExtraBytesParams(name=\"intensity\", type=np.uint16))\n",
    "    las.intensity = (pc.intensity * 65535).astype(np.uint16)  # Normalize if needed\n",
    "\n",
    "# Save to file\n",
    "updated_pc_path = '../../data/granite_dells/updated_point_cloud.las'\n",
    "las.write(updated_pc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
